Step 1: Finding the dataset: 

I am using the nfl_data_py module to predict the Win/Loss/Tie record of NFL teams for the 2023 season. The prediction will be based on historical data, player statistics, team performance metrics, and advanced analytics. Selecting the correct datasets ensures that we have comprehensive coverage of variables influencing team outcomes.

We are starting our journey by understanding what there is to offer, ths is done by help(nfl).
This will give us a base understanding of what datasets we have available and short description of said datasets.  
Please note, nfl_data_py is outdated and no longer being updated. We are using an old version of python.
import pandas as pd
import nfl_data_py as nfl
import matplotlib.pyplot as plt
import numpy as np
import statsmodels.api as sm
from scipy.stats import ttest_ind
from datetime import datetime
from datetime import date
import math 
from sklearn.linear_model import LogisticRegression
import seaborn as sb
help(nfl)

Step 2:
Choosing a dataset:

For this project, I have selected the import_pbp_data function from the nfl_data_py library, which provides play-by-play (PBP) data. The dataset is comprehensive, with a shape of (291,093 rows, 391 columns), offering a wealth of information for analysis. The large volume of data ensures that we can explore both granular details and higher-level trends, making it ideal for creating detailed insights.

The pbp dataset includes extensive game-related metrics and team-specific statistics, which makes it highly versatile. It connects directly to the team category while allowing full control over the data. This flexibility empowers us to engineer custom features or aggregated summaries at various levels, such as weekly, seasonal, or multi-seasonal statistics.

By leveraging the pbp dataset, we can analyze individual plays, identify patterns, and build robust models for deriving actionable insights. Its richness and flexibility make it the best choice for detailed exploration and in-depth analysis in the context of football performance and strategy.
#Grab Data
years = range(2018, 2024)
pbp_data = pd.concat([nfl.import_pbp_data(years)])

pbp_data.head()
pbp_data.shape
pbp_data.dtypes

We are only looking at regular season games and want to bring in month/day/year attributes. 
pbp =pbp_data[pbp_data['season_type'] == 'REG']
pbp['game_date'] = pd.to_datetime(pbp['game_date']) 
pbp['year'] = pbp['game_date'].dt.year
pbp['month'] = pbp['game_date'].dt.month
pbp['day'] = pbp['game_date'].dt.day


Manually split drive_time_of_poseession to seconds so we can compute later on. We wil convert N/A's/Nulls to 0's.
pbp['drive_time_of_possession_seconds'] = pbp['drive_time_of_possession'].apply(
    lambda x: int(x.split(':')[0]) * 60 + int(x.split(':')[1]) if pd.notnull(x) else 0
)

pbp['total_home_epa'] = pbp['total_home_epa'].fillna(0)
pbp['total_away_epa'] = pbp['total_away_epa'].fillna(0)
pbp['home_wp_post'] = pbp['home_wp_post'].fillna(0)
pbp['away_wp_post'] = pbp['away_wp_post'].fillna(0)

In order to get our target column, we need to identify who wins and loses each game. Below we are identifying and creating a new column called 'win' which is derived from away score and home score columns.  
def calculate_win(row):
    if row['home_score'] > row['away_score']:
        return {'home_team': 1, 'away_team': 0}
    elif row['home_score'] < row['away_score']:
        return {'home_team': 0, 'away_team': 1} 
    else:
        return {'home_team': 0, 'away_team': 0} 

pbp['home_win'] = pbp.apply(lambda row: calculate_win(row)['home_team'], axis=1)
pbp['away_win'] = pbp.apply(lambda row: calculate_win(row)['away_team'], axis=1)

Create a new DataFrame for game-level win aggregation for later usage when implementing target value. 
Along with that, we decided to drop all ties from the pbp dataset. 
game_results = pbp[['game_id', 'home_team', 'away_team', 'home_win', 'away_win','total_home_epa', 'total_away_epa', 'home_wp_post', 'away_wp_post','home_score','away_score']].drop_duplicates()
game_results = game_results[
    ~((game_results['home_win'] == 0) & (game_results['away_win'] == 0))
].reset_index(drop=True)

game_results    
Add in additional columns based on away and home columns (not determined by column: postplay), which will act as features. 

pbp_agg = game_results.groupby(['game_id', 'home_team', 'away_team']).agg({
    'home_score': 'max',
    'away_score': 'max',
    'home_win': 'max',  
    'away_win': 'max',
    'total_home_epa': 'mean',
    'total_away_epa': 'mean',
    'home_wp_post': 'mean',
    'away_wp_post': 'mean'
}).reset_index()

pbp_agg
split home and away results to then concat them into one column for team level stats. 
home_results = pbp_agg[['game_id', 'home_team', 'home_win', 'total_home_epa', 'home_wp_post','home_score']].rename(
    columns={
        'home_team': 'team',
        'home_win': 'win',
        'total_home_epa': 'epa',
        'home_wp_post': 'win_probability',
        'home_score': 'score'
    }
)

away_results = pbp_agg[['game_id', 'away_team', 'away_win', 'total_away_epa', 'away_wp_post','away_score']].rename(
    columns={
        'away_team': 'team',
        'away_win': 'win',
        'total_away_epa': 'epa',
        'away_wp_post': 'win_probability',
        'away_score': 'score'
    }
)

team_results = pd.concat([home_results, away_results])
team_results['game_team_id'] = team_results['game_id'] + "_" + team_results['team']
team_results.reset_index(drop=True, inplace=True)
team_results
Combine datasets by unique ID. sum up by game or get the mean of game to add to our new data set. 
pbp_pr= pbp[(pbp['pass'] == 1) | (pbp['rush'] == 1)]
pbp_avg = pbp_pr.groupby(['season','posteam','week','game_id']).agg({
    'passing_yards': 'mean',
    'complete_pass': 'sum',
    'pass': 'sum',
    'rushing_yards': 'mean',
    'rush': 'sum',
    'incomplete_pass' : 'sum',
    'interception':'sum',
    'third_down_converted': 'sum',
    'third_down_failed': 'sum',
    'fourth_down_converted': 'sum',
    'fourth_down_failed': 'sum',
    'first_down_pass': 'sum',
    'first_down_rush': 'sum',
    'fumble': 'sum',
    'drive_time_of_possession_seconds': 'mean',
}).reset_index()

pbp_avg = pbp_avg.rename(columns={
    'pass': 'passing_attempts',
    'rush': 'rushing_attempts',
    'rushing_yards': 'rush_yards_avg',
    'passing_yards': 'passing_yards_avg',
    'posteam':'team',
    'drive_time_of_possession_seconds':'drive_time_of_possession_seconds_avg'
})


pbp_avg['completion_percentage'] = (pbp_avg['complete_pass'] / pbp_avg['passing_attempts']) * 100
pbp_avg['pass_attempt_percentage'] = pbp_avg['passing_attempts'] / (pbp_avg['passing_attempts'] + pbp_avg['rushing_attempts']) * 100
pbp_avg['rush_attempt_percentage'] = pbp_avg['rushing_attempts'] / (pbp_avg['passing_attempts'] + pbp_avg['rushing_attempts']) * 100

pbp_avg['game_team_id'] = pbp_avg['game_id'] + "_" + pbp_avg['team']

pbp_avg = pbp_avg.merge(team_results, on='game_team_id', how='left')

pbp_avg.drop(columns=['game_id_y', 'team_y'], inplace=True)

pbp_avg


pbp_avg.dtypes
correclation coefficient to see what features impact the target value (wins) the most. 
Here is what we can tell, the mean of win_probability and epa have the greatest correlation to whether a team will win or not. This isn't a complete shock to me as the higher the mean of either one of these values indicates that they are making smarter and better plays throughout the game. 
Score is next on this list, this indicates that if a team scores more, they are more likely to win, also not a suprise. 
The real suprise is rush_attempt_percentage and rushing_attempts, althought not a great correlation, it is worth diving into. I will also say the same for pass_attempt_percentage / passing_attempt.
pbp_num = pbp_avg.select_dtypes(include=['float32', 'int64','float64'])
pbp_num_corr = pbp_num.corr()['win'].drop(['win','week','season'])
pbp_num_corr = pbp_num.corr()['win']
top_features = pbp_num_corr[abs(pbp_num_corr) > .25].sort_values(ascending=False) #displays pearsons correlation coefficient greater than 0.5
print("There is {} strongly correlated values with wins:\n{}".format(len(top_features), top_features))

Check for nulls. Ended up finding some Nulls because I only removed ties in one of the datasets so when I merged the data they showed back up. Cleared them and plotted, everything looks good. 

pbp_avg = pbp_avg.dropna(how='any')
Nulls = pbp_avg[pbp_avg['score'].isnull()]
Nulls


total = pbp_avg.isnull().sum().sort_values(ascending=False)
total_select = total.head(20)
total_select.plot(kind="bar", figsize = (8,6), fontsize = 10)

plt.xlabel("Columns", fontsize = 20)
plt.ylabel("Count", fontsize = 20)
plt.title("Total Missing Values", fontsize = 20)
Visuals below to verify or deny correlcation coefficient we saw earlier. 
numeric_cols = [col for col in pbp_avg.columns if pbp_avg[col].dtype in ['float64', 'float32', 'int64']]

n_cols = 3  
n_rows = math.ceil(len(numeric_cols) / n_cols)

fig, axes = plt.subplots(n_rows, n_cols, figsize=(n_cols * 5, n_rows * 4))
axes = axes.flatten()

for i, col in enumerate(numeric_cols):
    sb.boxplot(x='win', y=col, data=pbp_avg, ax=axes[i])
    axes[i].set_title(f'Boxplot of {col} by Win')
    axes[i].set_xlabel('Win (1) / Loss (0)')
    axes[i].set_ylabel(col)


for j in range(len(numeric_cols), len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()



feature = 'rushing_attempts'  
X = pbp_avg[[feature]].values
y = pbp_avg['win'].values

model = LogisticRegression()
model.fit(X, y)


x_vals = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)
y_vals = model.predict_proba(x_vals)[:, 1]

plt.figure(figsize=(8, 5))
plt.scatter(X, y, alpha=0.5, label='Data')
plt.plot(x_vals, y_vals, color='red', label='Logistic Regression Curve')
plt.title(f'Logistic Regression Fit for {feature}')
plt.xlabel(feature)
plt.ylabel('Win Probability')
plt.legend()
plt.show()

Hypothesis #1:

T-Test
Null Hypothesis: There is no significant difference in rushing attempts between winning and losing games.

Alternative Hypothesis: There is a significant difference in rushing attempts between winning and losing games 

Conclusion:
The p-value is far below 0.05, so we reject the null hypothesis.
Rushing attempts differ significantly between wins and losses. This implies that rushing plays a meaningful role in game outcomes.

add_value = 'rushing_attempts' 
wins = pbp_avg[pbp_avg['win'] == 1][add_value]
losses = pbp_avg[pbp_avg['win'] == 0][add_value]

t_stat, p_value = ttest_ind(wins, losses, equal_var=False)
print(f"T-Statistic: {t_stat}, P-Value: {p_value}")

if p_value < 0.05:
    print(f"Significant difference: {add_value} impacts game outcomes.")
else:
    print(f"No significant difference: {add_value} does not impact game outcomes.")

T-Test Explanation:
Split target column into wins and losses. 
Identify the amount of rushing attempts between the split dataset and conducts and independent t-test to compare the means of the two groups. 
P-value less than .05 means the feature is significant in determing the target value or rejecting the null hypothesis. (does not mean it is a reason why a team wins (many factors)) 
T-Statistic shows us the how many standard deviations the difference in means is from zero. Large t-stat suggest a strong seperation/difference between groups.

Cohen's d effect size of 1.0958 shows that rushing_attempts has a substantial impact on win values. 
#For my memory: 
# Calculate Cohen's d
mean_wins = wins.mean()
mean_losses = losses.mean()
std_wins = wins.std()
std_losses = losses.std()
n_wins = len(wins)
n_losses = len(losses)

# Pooled standard deviation
pooled_std = np.sqrt(((n_wins - 1) * std_wins**2 + (n_losses - 1) * std_losses**2) / (n_wins + n_losses - 2))

# Cohen's d final product
cohens_d = (mean_wins - mean_losses) / pooled_std
print(f"Cohen's d (effect size): {cohens_d}")
Isolating and revisulazing rushing_attempts vs win column by using a boxplot to solidify our answer to reject the null hypthoesis. 

pbp_avg['win'] = pbp_avg['win'].astype(int)

plt.figure(figsize=(8, 6))
sb.boxplot(x='win', y=add_value, data=pbp_avg, palette={'0': 'green', '1': 'red'})
plt.title(f'Comparison of {add_value} (Rushing Attempts) Between Winning and Losing Teams')
plt.xlabel('Win (1) / Loss (0)')
plt.ylabel(f'{add_value} (Rushing Attempts)')
plt.show()

Even though the increase in rushing_attempts means a higher chance of winning. This does not indicate that because a team runs the ball more, they are gonig to win. 
We would need to dive deeper to find out why. Is it because a team is close to winning the game and needs to run the clock out by running the ball? Thus making the other team need to throw more and cause this disperity between win/loss vs rushing attmpts.
Or does it indicate a teams ability to control the game and establish the run game (very important in football) early on. 

feature = 'rushing_attempts'
X = pbp_avg[[feature]]  
y = pbp_avg['win']  


model = LogisticRegression()
model.fit(X, y)

rushing_value = 27
predicted_prob = model.predict_proba([[rushing_value]])[:, 1]
print(f"Probability of winning with {rushing_value} rushing attempts: {predicted_prob[0]}")

Hypothesis 2: Teams that control time of possession are more likely to win.

Rationale: Time of possession can be an important factor in controlling the flow of the game. Teams that have the ball longer can dictate the pace, wear down the opposition, and have more opportunities to score, which may increase their likelihood of winning.
Hypothesis 3: Teams with higher passing attempt percentages are less likely to win.
Rationale: A high passing attempt percentage may indicate a team is playing from behind and needs to rely on the passing game to catch up, which might correlate with losing. Conversely, teams in the lead may lean more on the running game to manage the clock, resulting in fewer passing attempts.
Next Steps:
1: Expand Feature Analysis: Add new features or investigate additional columns that might have strong correlations with game outcomes, such as advanced metrics on offensive or defensive performance.

2:Include Defensive Metrics: Explore defensive statistics like sacks, interceptions, or opponent passing/rushing yards to see how they correlate with wins. This would provide a more balanced perspective on the factors influencing game outcomes.

3:Time-Series Analysis: Look into trends over time, such as how rushing or passing attempts evolve during a game, especially in different game states (e.g., leading or trailing). Could create new datasets to look at this differently, specifically play by play data. 

4.Clustering: Use clustering algorithms to group teams or games based on similar play styles or metrics, which might reveal hidden patterns.
Feature Engineering: Create composite metrics (e.g., turnover margin, yards per play) to capture more nuanced insights.

5:Feature Engineering: Create composite metrics (e.g., turnover margin, yards per play) to capture more nuanced insights.
Thoughts on data:
The dataset was relatively clean and provided the necessary variables for analysis, but it required significant preprocessing to make it usable. This included handling null values, creating new columns for better feature representation, and filtering out irrelevant data. While the data is comprehensive, its size and complexity introduce noise and increase the time needed to explore and interpret patterns. Moreover, the lack of defensive metrics limited the scope of analysis to primarily offensive statistics.

Request for Additional Data: To gain a deeper understanding, defensive metrics (e.g., opponent rushing attempts, turnovers forced) and situational data (e.g., time left in the game, score margin) would be invaluable. Adding these dimensions could help uncover whether certain play styles or game management strategies lead to consistent success.


